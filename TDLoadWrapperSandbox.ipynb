{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDLoad wapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_int</th>\n",
       "      <th>c_float</th>\n",
       "      <th>c_bool</th>\n",
       "      <th>c_string</th>\n",
       "      <th>c_list</th>\n",
       "      <th>c_timedelta</th>\n",
       "      <th>c_date</th>\n",
       "      <th>c_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>00:01:02</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01 10:10:10.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>b</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>00:03:01</td>\n",
       "      <td>1998-12-02</td>\n",
       "      <td>2020-01-01 10:10:10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>True</td>\n",
       "      <td>c</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "      <td>05:02:01</td>\n",
       "      <td>1980-04-01</td>\n",
       "      <td>2020-01-01 10:10:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>d</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>01:01:08</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>2020-01-01 10:10:05.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_int  c_float  c_bool c_string     c_list c_timedelta     c_date  \\\n",
       "0      1      1.2    True        a  [1, 2, 3]    00:01:02 2020-01-01   \n",
       "1      2     -0.3   False        b  [4, 5, 6]    00:03:01 1998-12-02   \n",
       "2      3      0.9    True        c  [7, 8, 9]    05:02:01 1980-04-01   \n",
       "3      4      1.5   False        d  [1, 2, 3]    01:01:08 2008-01-01   \n",
       "\n",
       "              c_timestamp  \n",
       "0 2020-01-01 10:10:10.090  \n",
       "1 2020-01-01 10:10:10.000  \n",
       "2 2020-01-01 10:10:00.000  \n",
       "3 2020-01-01 10:10:05.000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sample data\n",
    "import pandas as pd\n",
    "df_sample = pd.DataFrame({\"c_int\":[1,2,3,4], \"c_float\":[1.2, -0.3, 0.9, 1.5],\n",
    "                         \"c_bool\":[True,False,True,False], \n",
    "                          \"c_string\":[\"a\", \"b\", \"c\", \"d\"],\n",
    "                         \"c_list\":[[1,2,3], [4,5,6], [7,8,9], [1,2,3]],\n",
    "                         \"c_timedelta\":pd.to_timedelta([\"00:01:02\", \"00:03:01\",\n",
    "                                                     \"05:02:01\", \"1:01:08\"]),\n",
    "                         \"c_date\":pd.to_datetime([\"2020/1/1\",\"1998/12/2\",\n",
    "                                                   \"1980/4/1\",\"2008/1/1\"]),\n",
    "                         \"c_timestamp\":pd.to_datetime([\"2020/1/1 10:10:10.09\",\n",
    "                                                   \"2020/1/1 10:10:10\",\n",
    "                                                   \"2020/1/1 10:10\",\n",
    "                                                   \"2020/1/1 10:10:05\"])})\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. save csv\n",
    "source_path = \"tmp_file.csv\"\n",
    "tmp_csv_filename = source_path\n",
    "df_sample.to_csv(tmp_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create table\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"teradata://dbc:dbc@192.168.43.24:1025/tdwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'int64'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[\"c_int\"].dtype.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int64', 'float64', 'bool', 'object', 'object', 'object', 'object', 'object']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col.dtype.name for _, col in df_sample.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Integer, Float, String\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_bool</th>\n",
       "      <th>c_timedelta</th>\n",
       "      <th>c_date</th>\n",
       "      <th>c_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:01:02.000000000</td>\n",
       "      <td>2020-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0 days 00:03:01.000000000</td>\n",
       "      <td>1998-12-02 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0 days 05:02:01.000000000</td>\n",
       "      <td>1980-04-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0 days 01:01:08.000000000</td>\n",
       "      <td>2008-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:05.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  c_bool                c_timedelta                   c_date  \\\n",
       "0   True  0 days 00:01:02.000000000  2020-01-01 00:00:00.000   \n",
       "1  False  0 days 00:03:01.000000000  1998-12-02 00:00:00.000   \n",
       "2   True  0 days 05:02:01.000000000  1980-04-01 00:00:00.000   \n",
       "3  False  0 days 01:01:08.000000000  2008-01-01 00:00:00.000   \n",
       "\n",
       "               c_timestamp  \n",
       "0  2020-01-01 10:10:10.090  \n",
       "1  2020-01-01 10:10:10.000  \n",
       "2  2020-01-01 10:10:00.000  \n",
       "3  2020-01-01 10:10:05.000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[[\"c_bool\", \"c_timedelta\", \"c_date\", \"c_timestamp\"]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_parser(series:pd.Series)->str:\n",
    "    if series.dtype.name == \"int64\":\n",
    "        parsed_dtype = \"Integer\"\n",
    "    elif series.dtype.name == \"float64\":\n",
    "        parsed_dtype = \"float\"\n",
    "    else:\n",
    "        max_length = max(series.astype(str).str.len()) + 5 # add 5 length\n",
    "        parsed_dtype = f\"varchar({max_length})\"\n",
    "    return parsed_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Integer',\n",
       " 'float',\n",
       " 'varchar(10)',\n",
       " 'varchar(6)',\n",
       " 'varchar(14)',\n",
       " 'varchar(30)',\n",
       " 'varchar(28)',\n",
       " 'varchar(28)']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = [dtype_parser(col) for _, col in df_sample.iteritems()]\n",
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"tdwork.df_sample\"\n",
    "column_query_part = \"\\n    ,\".join(\n",
    "    [f\"{name} {dtype_parser(col)}\" for name, col in df_sample.iteritems()])\n",
    "index_query_part = \"no primary index\"\n",
    "\n",
    "query = \"\"\"\n",
    "create table {tablename} (\n",
    "    {column_query_part}\n",
    ") {index_query_part}\n",
    "\"\"\".format(tablename=tablename, column_query_part=column_query_part,\n",
    "           index_query_part=index_query_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create table tdwork.df_sample (\n",
      "    c_int Integer\n",
      "    ,c_float float\n",
      "    ,c_bool varchar(10)\n",
      "    ,c_string varchar(6)\n",
      "    ,c_list varchar(14)\n",
      "    ,c_timedelta varchar(30)\n",
      "    ,c_date varchar(28)\n",
      "    ,c_timestamp varchar(28)\n",
      ") no primary index\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"\"\"\n",
    "select * from dbc.TablesV\n",
    "where TableKind = 'T'\n",
    "and TableName = '{tablename}'\n",
    "and DatabaseName = '{working_db}'\"\"\".format(working_db=working_db,\n",
    "                                           tablename=tablename)\n",
    "                  ,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_if_exists(tablename:str, dbname:str)->None:\n",
    "    table_list = pd.read_sql_query(\n",
    "        \"\"\"select * from dbc.TablesV\n",
    "        where TableName = '{tablename}'\n",
    "        and DatabaseName = '{dbname}'\"\"\".format(\n",
    "            dbname=dbname, tablename=tablename)\n",
    "        ,engine)\n",
    "    if len(table_list) > 0:\n",
    "        engine.execute(f\"drop table {dbname}.{tablename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_if_exists(\"df_sample\", \"tdwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x1153243a0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Request Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREATE MULTISET TABLE tdwork.df_sample ,FALLBA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Request Text\n",
       "0  CREATE MULTISET TABLE tdwork.df_sample ,FALLBA..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(f\"show table {tablename}\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. copy data to server\n",
    "import paramiko\n",
    "import scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramiko.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"~/tmp_file.csv\"\n",
    "\n",
    "with paramiko.SSHClient() as sshc:\n",
    "    sshc.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    sshc.connect(hostname=\"192.168.43.24\", port=22, username=\"root\",\n",
    "                 password=\"root\", pkey=None)\n",
    "    \n",
    "    with scp.SCPClient(sshc.get_transport()) as scpc:\n",
    "        scpc.put(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tdload -f ~/tmp_file.csv -t tdwork.df_sample -h 192.168.43.24 --TargetWorkingDatabase tdwork -u dbc -p dbc --SourceSkipRows 1 --DCPQuotedData 'Optional' jobload_tmp2\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname = \"192.168.43.24\"\n",
    "username = \"dbc\"\n",
    "password = \"dbc\"\n",
    "jobname = \"jobload_tmp2\"\n",
    "tablename = \"tdwork.df_sample\"\n",
    "working_db = \"tdwork\"\n",
    "options = \"--SourceSkipRows 1\" + \" --DCPQuotedData 'Optional'\"\n",
    "\n",
    "tdload_command = (f\"tdload -f {target_path} -t {tablename} -h {hostname}\"\n",
    "                  + f\" --TargetWorkingDatabase {working_db}\"\n",
    "                  + f\" -u {username} -p {password} {options} {jobname}\")\n",
    "tdload_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_if_exists(\"df_sample_ET\", \"tdwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_if_exists(\"df_sample_UV\", \"tdwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_int,c_float,c_bool,c_string,c_list,c_timedelta,c_date,c_timestamp\n",
      "1,1.2,True,a,\"[1, 2, 3]\",0 days 00:01:02.000000000,2020-01-01 00:00:00.000,2020-01-01 10:10:10.090\n",
      "2,-0.3,False,b,\"[4, 5, 6]\",0 days 00:03:01.000000000,1998-12-02 00:00:00.000,2020-01-01 10:10:10.000\n",
      "3,0.9,True,c,\"[7, 8, 9]\",0 days 05:02:01.000000000,1980-04-01 00:00:00.000,2020-01-01 10:10:00.000\n",
      "4,1.5,False,d,\"[1, 2, 3]\",0 days 01:01:08.000000000,2008-01-01 00:00:00.000,2020-01-01 10:10:05.000\n"
     ]
    }
   ],
   "source": [
    "!cat tmp_file.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobload_tmp2-13.out\n",
      "\n",
      "Job id is jobload_tmp2-13, running on TDExpress1620_Sles11\n",
      "\n",
      "Found CheckPoint file: /opt/teradata/client/16.20/tbuild/checkpoint/jobload_tmp2LVCP\n",
      "\n",
      "This is a restart job; it restarts at step MAIN_STEP.\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "$FILE_READER[1]: TPT19011 Instance 1 restarting.\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-10966'\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/root/tmp_file.csv'.\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.df_sample'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      4\n",
      "\n",
      "$LOAD: Total Rows Applied:            4\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.000\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   2 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   4 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase: < 1 second\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end: < 1 second\n",
      "\n",
      "$LOAD: Total processor time used = '0.096006 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 02:10:33 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 02:10:39 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobload_tmp2 completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 02:10:33 2020\n",
      "\n",
      "Job end:   Sat Jan 25 02:10:39 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. load data to table\n",
    "\n",
    "with paramiko.SSHClient() as sshc:\n",
    "    sshc.load_system_host_keys()\n",
    "    sshc.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    sshc.connect(hostname=\"192.168.43.24\", port=22, username=\"root\",\n",
    "                 password=\"root\")\n",
    "    \n",
    "    stdin, stdout, stderr = sshc.exec_command(tdload_command)\n",
    "    for line in stdout:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_int</th>\n",
       "      <th>c_float</th>\n",
       "      <th>c_bool</th>\n",
       "      <th>c_string</th>\n",
       "      <th>c_list</th>\n",
       "      <th>c_timedelta</th>\n",
       "      <th>c_date</th>\n",
       "      <th>c_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>0 days 00:01:02.000000000</td>\n",
       "      <td>2020-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>b</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>0 days 00:03:01.000000000</td>\n",
       "      <td>1998-12-02 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>True</td>\n",
       "      <td>c</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "      <td>0 days 05:02:01.000000000</td>\n",
       "      <td>1980-04-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>d</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>0 days 01:01:08.000000000</td>\n",
       "      <td>2008-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:05.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_int  c_float c_bool c_string     c_list                c_timedelta  \\\n",
       "0      1      1.2   True        a  [1, 2, 3]  0 days 00:01:02.000000000   \n",
       "1      2     -0.3  False        b  [4, 5, 6]  0 days 00:03:01.000000000   \n",
       "2      3      0.9   True        c  [7, 8, 9]  0 days 05:02:01.000000000   \n",
       "3      4      1.5  False        d  [1, 2, 3]  0 days 01:01:08.000000000   \n",
       "\n",
       "                    c_date              c_timestamp  \n",
       "0  2020-01-01 00:00:00.000  2020-01-01 10:10:10.090  \n",
       "1  1998-12-02 00:00:00.000  2020-01-01 10:10:10.000  \n",
       "2  1980-04-01 00:00:00.000  2020-01-01 10:10:00.000  \n",
       "3  2008-01-01 00:00:00.000  2020-01-01 10:10:05.000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"select * from tdwork.df_sample\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. load data to table\n",
    "\n",
    "with paramiko.SSHClient() as sshc:\n",
    "    sshc.load_system_host_keys()\n",
    "    sshc.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    sshc.connect(hostname=\"192.168.43.24\", port=22, username=\"root\",\n",
    "                 password=\"root\")\n",
    "    \n",
    "    stdin, stdout, stderr = sshc.exec_command(tdload_command)\n",
    "    for line in stdout:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlalchemy.engine.base.Engine"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramiko.client.SSHClient"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sshc.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idx'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"index\", \"idx\", \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 41), match='Job step MAIN_STEP completed successfully'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableIsExist(tablename:str, dbname:str,\n",
    "                 engine:sqlalchemy.engine.base.Engine)->bool:\n",
    "    table_list = pd.read_sql_query(\n",
    "        \"\"\"select * from dbc.TablesV\n",
    "        where TableName = '{tablename}'\n",
    "        and DatabaseName = '{dbname}'\"\"\".format(\n",
    "            dbname=dbname, tablename=tablename)\n",
    "        ,engine)\n",
    "    return True if len(table_list) > 0 else False\n",
    "\n",
    "def dropIfExists(tablename:str, dbname:str,\n",
    "                engine:sqlalchemy.engine.base.Engine)->None:\n",
    "    if tableIsExist(tablename,dbname,engine):\n",
    "        engine.execute(f\"drop table {dbname}.{tablename}\")\n",
    "        print(f\"droped table {dbname}.{tablename}\")\n",
    "        \n",
    "def dtypeParser(series:pd.Series)->str:\n",
    "    if series.dtype.name == \"int64\":\n",
    "        parsed_dtype = \"Integer\"\n",
    "    elif series.dtype.name == \"float64\":\n",
    "        parsed_dtype = \"float\"\n",
    "    else:\n",
    "        max_length = max(series.astype(str).str.len()) + 5 # add 5 length\n",
    "        parsed_dtype = f\"varchar({max_length})\"\n",
    "    return parsed_dtype\n",
    "\n",
    "def renameColumns(series:pd.Series)->pd.Series():\n",
    "    res = (series.astype(str).str\n",
    "           .replace(r\"\\s\",\"_\", regex=True)\n",
    "           .replace(r\"\\W\",\"\", regex=True)\n",
    "           .replace(r\"^(\\d)\", \"col_\\\\1\", regex=True)\n",
    "           .replace(\"index\", \"idx\"))\n",
    "    return res\n",
    "        \n",
    "def createTable(df:pd.DataFrame, \n",
    "                engine:sqlalchemy.engine.base.Engine,\n",
    "                tablename:str, dbname:str=None,\n",
    "                overwrite:bool=False, indexList:list=None,\n",
    "                isIndexUnique:bool=True)->None:\n",
    "    if dbname is None: dbname = engine.url.database\n",
    "    new_names = renameColumns(df.columns.to_series())\n",
    "    col_dtype = [dtypeParser(col) for _, col in df.iteritems()]\n",
    "    column_query_part = \"\\n    ,\".join(\n",
    "        [f\"{name} {dtype}\" for name, dtype \n",
    "         in zip(new_names, col_dtype)]\n",
    "    )\n",
    "    if indexList:\n",
    "        idxSeries = pd.Series(indexList).astype(str)\n",
    "        idx_str = \",\".join(renameColumns(idxSeries))\n",
    "        q_unique = \"unique \" if isIndexUnique else \"\"\n",
    "        index_query_part = q_unique + f\"primary index ({idx_str})\"\n",
    "    else:\n",
    "        index_query_part = \"no primary index\"\n",
    "    query = \"\"\"\n",
    "    create table {dbname}.{tablename} (\n",
    "        {column_query_part}\n",
    "    ) {index_query_part}\n",
    "    \"\"\".format(tablename=tablename, dbname=dbname,\n",
    "               column_query_part=column_query_part,\n",
    "               index_query_part=index_query_part)\n",
    "    if overwrite:\n",
    "        dropIfExists(tablename, dbname, engine)\n",
    "    else:\n",
    "        if tableIsExist(tablename, dbname, engine):\n",
    "            raise ValueError(f\"{tablename}.{dbname} is already exists.\")\n",
    "    engine.execute(query)\n",
    "    print(\"created table \\n\" + query)\n",
    "\n",
    "def connectSSH(hostname:str, username:str, password:str=None,\n",
    "               port:int=22, keyPath:str=None)->paramiko.client.SSHClient:\n",
    "    if keyPath is not None:\n",
    "        k = paramiko.RSAKey.from_private_key_file(keyPath)\n",
    "    else:\n",
    "        k = None\n",
    "    sshc = paramiko.SSHClient()\n",
    "    sshc.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    sshc.connect(hostname, port, username, password, pkey=k) \n",
    "    return sshc\n",
    "    \n",
    "def uploadFile(sourcePath:str, targetPath:str,\n",
    "               sshc:paramiko.client.SSHClient)->None:\n",
    "    with scp.SCPClient(sshc.get_transport()) as scpc:\n",
    "        scpc.put(sourcePath, targetPath)\n",
    "            \n",
    "def tdloadViaSSH(engine:sqlalchemy.engine.base.Engine,\n",
    "                 sshc:paramiko.client.SSHClient, \n",
    "                 tablename:str, targetFile:str, dbname:str=None, \n",
    "                 jobname:str=\"jobtmp\", skipRowNum:int=0,\n",
    "                 verbose=True)->None:\n",
    "    \n",
    "    if dbname is None: dbname = engine.url.database    \n",
    "    options = \"--DCPQuotedData 'Optional'\"\n",
    "    if skipRowNum > 0:\n",
    "        options += f\" --SourceSkipRows {skipRowNum}\"\n",
    "        \n",
    "    tdload_command = (f\"tdload -f {targetFile} -t {dbname}.{tablename}\"\n",
    "                      + f\" -h {engine.url.host} -u {engine.url.username}\"\n",
    "                      + f\" -p {engine.url.password}\"\n",
    "                      + f\" --TargetWorkingDatabase {dbname}\"\n",
    "                      + f\" {options} {jobname}\")\n",
    "    \n",
    "    dropIfExists(tablename+\"_ET\", dbname, engine)\n",
    "    dropIfExists(tablename+\"_UV\", dbname, engine)\n",
    "    \n",
    "    stdin, stdout, stderr = sshc.exec_command(tdload_command)\n",
    "    for line in stdout:\n",
    "        if verbose:\n",
    "            print(line)\n",
    "        else:\n",
    "            if re.match(r\".*(Total Rows|successfully).*\", line): print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_int</th>\n",
       "      <th>c_float</th>\n",
       "      <th>c_bool</th>\n",
       "      <th>c_string</th>\n",
       "      <th>c_list</th>\n",
       "      <th>c_timedelta</th>\n",
       "      <th>c_date</th>\n",
       "      <th>c_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>True</td>\n",
       "      <td>c</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "      <td>0 days 05:02:01.000000000</td>\n",
       "      <td>1980-04-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>0 days 00:01:02.000000000</td>\n",
       "      <td>2020-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>d</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>0 days 01:01:08.000000000</td>\n",
       "      <td>2008-01-01 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:05.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>b</td>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>0 days 00:03:01.000000000</td>\n",
       "      <td>1998-12-02 00:00:00.000</td>\n",
       "      <td>2020-01-01 10:10:10.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_int  c_float c_bool c_string     c_list                c_timedelta  \\\n",
       "0    3.0      0.9   True        c  [7, 8, 9]  0 days 05:02:01.000000000   \n",
       "1    1.0      1.2   True        a  [1, 2, 3]  0 days 00:01:02.000000000   \n",
       "2    4.0      1.5  False        d  [1, 2, 3]  0 days 01:01:08.000000000   \n",
       "3    2.0     -0.3  False        b  [4, 5, 6]  0 days 00:03:01.000000000   \n",
       "\n",
       "                    c_date              c_timestamp  \n",
       "0  1980-04-01 00:00:00.000  2020-01-01 10:10:00.000  \n",
       "1  2020-01-01 00:00:00.000  2020-01-01 10:10:10.090  \n",
       "2  2008-01-01 00:00:00.000  2020-01-01 10:10:05.000  \n",
       "3  1998-12-02 00:00:00.000  2020-01-01 10:10:10.000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"select * from df_sample3\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tdwork'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.url.database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbc'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.url.password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'192.168.43.24'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.url.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "teradatasql://dbc:***@192.168.43.24:1025/tdwork"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdload_df(df:pd.DataFrame, engine:sqlalchemy.engine.base.Engine,\n",
    "              tablename:str, ssh_ip:str, ssh_username:str, dbname:str=None,\n",
    "              overwrite:bool=False, ssh_password:str=None, ssh_keypath:str=None,\n",
    "              ssh_folder:str=None, saveIndex=False, indexList:list=None,\n",
    "              isIndexUnique:bool=True, verbose=True)->None:\n",
    "\n",
    "    # 1. save csv\n",
    "    if dbname is None: dbname = engine.url.database\n",
    "    sourceName = f\"tmp_{dbname}_{tablename}_{time.time():.0f}.csv\"\n",
    "    if saveIndex: df = df.reset_index()\n",
    "    df.to_csv(sourceName, index=False)\n",
    "    df_tmp = pd.read_csv(sourceName)\n",
    "    \n",
    "    # 2. create table\n",
    "    createTable(df_tmp, engine, tablename, dbname, overwrite, indexList, isIndexUnique)\n",
    "\n",
    "    # 3. copy file with scp\n",
    "    if ssh_folder is None: ssh_folder = \"~\"\n",
    "    targetPath = \"/\" + sourceName\n",
    "    \n",
    "    sshc = connectSSH(ssh_ip, ssh_username, ssh_password, keyPath=ssh_keypath)\n",
    "    uploadFile(sourceName, targetPath, sshc)\n",
    "\n",
    "    # 4. load file with tdload\n",
    "    tdloadViaSSH(engine=engine, sshc=sshc, tablename=tablename,\n",
    "                 targetFile=targetPath, dbname=dbname,\n",
    "                 skipRowNum=1, verbose=verbose)\n",
    "    \n",
    "    # 5. delete tmp files and close connection\n",
    "    os.remove(sourceName)\n",
    "    sftp = sshc.open_sftp()\n",
    "    sftp.remove(targetPath)\n",
    "    sshc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droped table tdwork.df_sample3\n",
      "created table \n",
      "\n",
      "    create table tdwork.df_sample3 (\n",
      "        c_int Integer\n",
      "    ,c_float float\n",
      "    ,c_bool varchar(10)\n",
      "    ,c_string varchar(6)\n",
      "    ,c_list varchar(14)\n",
      "    ,c_timedelta varchar(30)\n",
      "    ,c_date varchar(28)\n",
      "    ,c_timestamp varchar(28)\n",
      "    ) primary index (c_int)\n",
      "    \n",
      "$LOAD: Total Rows Sent To RDBMS:      4\n",
      "\n",
      "$LOAD: Total Rows Applied:            4\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdload_df(df_sample, engine, tablename=\"df_sample3\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\",\n",
    "          indexList=\"c_int\", isIndexUnique=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_boston, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, load_digits, load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_iris().get(\"sanada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(loader):\n",
    "    data = loader()\n",
    "    columns = data.get(\"feature_names\")\n",
    "    df = pd.DataFrame(data.data, columns=columns)\n",
    "    df[\"target\"] = pd.Series(data.target)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droped table tdwork.tdload_iris\n",
      "created table \n",
      "\n",
      "    create table tdwork.tdload_iris (\n",
      "        idx Integer\n",
      "    ,sepal_length_cm float\n",
      "    ,sepal_width_cm float\n",
      "    ,petal_length_cm float\n",
      "    ,petal_width_cm float\n",
      "    ,target Integer\n",
      "    ) unique primary index (idx)\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-42.out\n",
      "\n",
      "Job id is jobtmp-42, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-31535'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_iris_1579950674.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_iris'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      150\n",
      "\n",
      "$LOAD: Total Rows Applied:            150\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.002\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   2 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase:   1 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end: < 1 second\n",
      "\n",
      "$LOAD: Total processor time used = '0.084005 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 06:11:16 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 06:11:22 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 06:11:16 2020\n",
      "\n",
      "Job end:   Sat Jan 25 06:11:22 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris = make_df(load_iris)\n",
    "tdload_df(df_iris, engine, tablename=\"tdload_iris\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\",\n",
    "         saveIndex=True, indexList=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created table \n",
      "\n",
      "    create table tdwork.tdload_boston (\n",
      "        CRIM float\n",
      "    ,ZN float\n",
      "    ,INDUS float\n",
      "    ,CHAS float\n",
      "    ,NOX float\n",
      "    ,RM float\n",
      "    ,AGE float\n",
      "    ,DIS float\n",
      "    ,RAD float\n",
      "    ,TAX float\n",
      "    ,PTRATIO float\n",
      "    ,B float\n",
      "    ,LSTAT float\n",
      "    ,target float\n",
      "    ) no primary index\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-22.out\n",
      "\n",
      "Job id is jobtmp-22, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-21317'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_boston_1579943743.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_boston'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      506\n",
      "\n",
      "$LOAD: Total Rows Applied:            506\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.016\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   2 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase: < 1 second\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end:   1 second(s)\n",
      "\n",
      "$LOAD: Total processor time used = '0.096006 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 04:15:45 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 04:15:51 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 04:15:45 2020\n",
      "\n",
      "Job end:   Sat Jan 25 04:15:51 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_boston = make_df(load_boston)\n",
    "tdload_df(df_boston, engine, tablename=\"tdload_boston\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droped table tdwork.tdload_cancer\n",
      "created table \n",
      "\n",
      "    create table tdwork.tdload_cancer (\n",
      "        mean_radius float\n",
      "    ,mean_texture float\n",
      "    ,mean_perimeter float\n",
      "    ,mean_area float\n",
      "    ,mean_smoothness float\n",
      "    ,mean_compactness float\n",
      "    ,mean_concavity float\n",
      "    ,mean_concave_points float\n",
      "    ,mean_symmetry float\n",
      "    ,mean_fractal_dimension float\n",
      "    ,radius_error float\n",
      "    ,texture_error float\n",
      "    ,perimeter_error float\n",
      "    ,area_error float\n",
      "    ,smoothness_error float\n",
      "    ,compactness_error float\n",
      "    ,concavity_error float\n",
      "    ,concave_points_error float\n",
      "    ,symmetry_error float\n",
      "    ,fractal_dimension_error float\n",
      "    ,worst_radius float\n",
      "    ,worst_texture float\n",
      "    ,worst_perimeter float\n",
      "    ,worst_area float\n",
      "    ,worst_smoothness float\n",
      "    ,worst_compactness float\n",
      "    ,worst_concavity float\n",
      "    ,worst_concave_points float\n",
      "    ,worst_symmetry float\n",
      "    ,worst_fractal_dimension float\n",
      "    ,target Integer\n",
      "    ) no primary index\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-24.out\n",
      "\n",
      "Job id is jobtmp-24, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-21473'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_cancer_1579943808.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_cancer'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      569\n",
      "\n",
      "$LOAD: Total Rows Applied:            569\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.045\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   2 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase:   1 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end: < 1 second\n",
      "\n",
      "$LOAD: Total processor time used = '0.080004 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 04:16:51 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 04:16:57 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 04:16:51 2020\n",
      "\n",
      "Job end:   Sat Jan 25 04:16:57 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cancer = make_df(load_breast_cancer)\n",
    "tdload_df(df_cancer, engine, tablename=\"tdload_cancer\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created table \n",
      "\n",
      "    create table tdwork.tdload_diabetes (\n",
      "        age float\n",
      "    ,sex float\n",
      "    ,bmi float\n",
      "    ,bp float\n",
      "    ,s1 float\n",
      "    ,s2 float\n",
      "    ,s3 float\n",
      "    ,s4 float\n",
      "    ,s5 float\n",
      "    ,s6 float\n",
      "    ,target float\n",
      "    ) no primary index\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-25.out\n",
      "\n",
      "Job id is jobtmp-25, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-21584'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_diabetes_1579943909.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_diabetes'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      442\n",
      "\n",
      "$LOAD: Total Rows Applied:            442\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.031\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   2 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase: < 1 second\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end:   1 second(s)\n",
      "\n",
      "$LOAD: Total processor time used = '0.084005 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 04:18:31 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 04:18:37 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 04:18:31 2020\n",
      "\n",
      "Job end:   Sat Jan 25 04:18:37 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_diabetes = make_df(load_diabetes)\n",
    "tdload_df(df_diabetes, engine, tablename=\"tdload_diabetes\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created table \n",
      "\n",
      "    create table tdwork.tdload_wine (\n",
      "        alcohol float\n",
      "    ,malic_acid float\n",
      "    ,ash float\n",
      "    ,alcalinity_of_ash float\n",
      "    ,magnesium float\n",
      "    ,total_phenols float\n",
      "    ,flavanoids float\n",
      "    ,nonflavanoid_phenols float\n",
      "    ,proanthocyanins float\n",
      "    ,color_intensity float\n",
      "    ,hue float\n",
      "    ,od280od315_of_diluted_wines float\n",
      "    ,proline float\n",
      "    ,target Integer\n",
      "    ) no primary index\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-26.out\n",
      "\n",
      "Job id is jobtmp-26, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-21673'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_wine_1579943971.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_wine'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      178\n",
      "\n",
      "$LOAD: Total Rows Applied:            178\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.005\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase: < 1 second\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end: < 1 second\n",
      "\n",
      "$LOAD: Total processor time used = '0.104007 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 04:19:33 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 04:19:39 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 04:19:33 2020\n",
      "\n",
      "Job end:   Sat Jan 25 04:19:39 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wine = make_df(load_wine)\n",
    "tdload_df(df_wine, engine, tablename=\"tdload_wine\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created table \n",
      "\n",
      "    create table tdwork.tdload_digits (\n",
      "        col_0 float\n",
      "    ,col_1 float\n",
      "    ,col_2 float\n",
      "    ,col_3 float\n",
      "    ,col_4 float\n",
      "    ,col_5 float\n",
      "    ,col_6 float\n",
      "    ,col_7 float\n",
      "    ,col_8 float\n",
      "    ,col_9 float\n",
      "    ,col_10 float\n",
      "    ,col_11 float\n",
      "    ,col_12 float\n",
      "    ,col_13 float\n",
      "    ,col_14 float\n",
      "    ,col_15 float\n",
      "    ,col_16 float\n",
      "    ,col_17 float\n",
      "    ,col_18 float\n",
      "    ,col_19 float\n",
      "    ,col_20 float\n",
      "    ,col_21 float\n",
      "    ,col_22 float\n",
      "    ,col_23 float\n",
      "    ,col_24 float\n",
      "    ,col_25 float\n",
      "    ,col_26 float\n",
      "    ,col_27 float\n",
      "    ,col_28 float\n",
      "    ,col_29 float\n",
      "    ,col_30 float\n",
      "    ,col_31 float\n",
      "    ,col_32 float\n",
      "    ,col_33 float\n",
      "    ,col_34 float\n",
      "    ,col_35 float\n",
      "    ,col_36 float\n",
      "    ,col_37 float\n",
      "    ,col_38 float\n",
      "    ,col_39 float\n",
      "    ,col_40 float\n",
      "    ,col_41 float\n",
      "    ,col_42 float\n",
      "    ,col_43 float\n",
      "    ,col_44 float\n",
      "    ,col_45 float\n",
      "    ,col_46 float\n",
      "    ,col_47 float\n",
      "    ,col_48 float\n",
      "    ,col_49 float\n",
      "    ,col_50 float\n",
      "    ,col_51 float\n",
      "    ,col_52 float\n",
      "    ,col_53 float\n",
      "    ,col_54 float\n",
      "    ,col_55 float\n",
      "    ,col_56 float\n",
      "    ,col_57 float\n",
      "    ,col_58 float\n",
      "    ,col_59 float\n",
      "    ,col_60 float\n",
      "    ,col_61 float\n",
      "    ,col_62 float\n",
      "    ,col_63 float\n",
      "    ,target Integer\n",
      "    ) no primary index\n",
      "    \n",
      "Teradata Parallel Transporter Version 16.20.00.09 64-Bit\n",
      "\n",
      "The global configuration file '/opt/teradata/client/16.20/tbuild/twbcfg.ini' is used.\n",
      "\n",
      "   Log Directory: /opt/teradata/client/16.20/tbuild/logs\n",
      "\n",
      "   Checkpoint Directory: /opt/teradata/client/16.20/tbuild/checkpoint\n",
      "\n",
      "\n",
      "\n",
      "Job log: /opt/teradata/client/16.20/tbuild/logs/jobtmp-27.out\n",
      "\n",
      "Job id is jobtmp-27, running on TDExpress1620_Sles11\n",
      "\n",
      "Teradata Parallel Transporter Load Operator Version 16.20.00.09\n",
      "\n",
      "$LOAD: private log specified: LoadLog\n",
      "\n",
      "Teradata Parallel Transporter DataConnector Operator Version 16.20.00.09\n",
      "\n",
      "$FILE_READER[1]: DataConnector Producer operator Instances: 1\n",
      "\n",
      "$FILE_READER[1]: ECI operator ID: '$FILE_READER-22353'\n",
      "\n",
      "$FILE_READER[1]: Operator instance 1 processing file '/tmp_tdwork_tdload_digits_1579944520.csv'.\n",
      "\n",
      "$LOAD: connecting sessions\n",
      "\n",
      "$LOAD: preparing target table\n",
      "\n",
      "$LOAD: entering Acquisition Phase\n",
      "\n",
      "$LOAD: entering Application Phase\n",
      "\n",
      "$LOAD: Statistics for Target Table:  'tdwork.tdload_digits'\n",
      "\n",
      "$LOAD: Total Rows Sent To RDBMS:      1797\n",
      "\n",
      "$LOAD: Total Rows Applied:            1797\n",
      "\n",
      "$LOAD: Total Rows in Error Table 1:   0\n",
      "\n",
      "$LOAD: Total Rows in Error Table 2:   0\n",
      "\n",
      "$LOAD: Total Duplicate Rows:          0\n",
      "\n",
      "$FILE_READER[1]: Total files processed: 1.\n",
      "\n",
      "$LOAD: disconnecting sessions\n",
      "\n",
      "$LOAD: Performance metrics:\n",
      "\n",
      "$LOAD:     MB/sec in Acquisition phase: 0.202\n",
      "\n",
      "$LOAD:     Elapsed time from start to Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Acquisition phase:   3 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time in Application phase:   1 second(s)\n",
      "\n",
      "$LOAD:     Elapsed time from Application phase to end:   1 second(s)\n",
      "\n",
      "$LOAD: Total processor time used = '0.136009 Second(s)'\n",
      "\n",
      "$LOAD: Start : Sat Jan 25 04:28:42 2020\n",
      "\n",
      "$LOAD: End   : Sat Jan 25 04:28:50 2020\n",
      "\n",
      "Job step MAIN_STEP completed successfully\n",
      "\n",
      "Job jobtmp completed successfully\n",
      "\n",
      "Job start: Sat Jan 25 04:28:42 2020\n",
      "\n",
      "Job end:   Sat Jan 25 04:28:50 2020\n",
      "\n",
      "Teradata Load Utility Version 16.20.00.09 64-Bit\n",
      "\n",
      "TPT_INFRA: TPT05550: Warning: no Source File Delimiter specified, default \",\" will be used\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_digits= make_df(load_digits)\n",
    "tdload_df(df_digits, engine, tablename=\"tdload_digits\", overwrite=True,\n",
    "          ssh_ip=\"192.168.43.24\", ssh_username=\"root\", ssh_password=\"root\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn2vantage] *",
   "language": "python",
   "name": "conda-env-sklearn2vantage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
